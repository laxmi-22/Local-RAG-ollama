# ğŸ“„ Local RAG using Ollama

(Part of My RAG Learning Journey â€“ Post 3)

## ğŸ“Œ Context

This repository is part of my RAG learning journey, where Iâ€™m exploring different ways to design and implement Retrieval-Augmented Generation (RAG) systems.

In this phase, I focused on answering a practical question:

Can we build a complete RAG pipeline locally â€” without cloud APIs â€” while still maintaining retrieval quality and grounding?

This project demonstrates a Local RAG system using Ollama, designed to understand privacy, cost, and control trade-offs in real-world scenarios.

## ğŸ§  What This Project Demonstrates

- End-to-end local RAG pipeline
- PDF-based question answering
- Retrieval grounded strictly in document context
- Focus on preprocessing and retrieval quality, not UI

## ğŸ—ï¸ High-Level Flow

PDF Document
   â†“
Preprocessing (remove headers, footers, emojis)
   â†“
Text Chunking
   â†“
Embeddings (local â€“ Ollama)
   â†“
Vector Store (FAISS)
   â†“
Retriever (MMR)
   â†“
LLM (Ollama)
   â†“
Context-grounded Answer

## ğŸ” Key Learning (Core Insight)

Initially, I directly moved from PDF loading to chunking.
Through iteration, I realized that preprocessing PDFs before chunking â€” especially removing repetitive headers, footers, and emojis â€” significantly improves:

- Embedding quality
- Retrieval relevance
- Final answer accuracy

## â¡ï¸ Cleaner input â†’ better retrieval â†’ more reliable RAG output

This learning influenced the final design of this pipeline.

## ğŸ”§ Design Choices & Reasoning
## âœ” PDF Preprocessing

- Removes repetitive headers & footers
- Removes emojis and noise
- Reduces embedding pollution
- Improves semantic similarity during retrieval

## âœ” Chunking Strategy

- Recursive character-based splitting
- Chunk overlap to preserve context
- Page metadata retained

## âœ” Vector Store

FAISS (local, lightweight, fast)

## âœ” Retriever

## - MMR (Max Marginal Relevance) used to:

- Reduce redundant chunks
- Improve diversity in retrieved context
- Balance relevance vs overlap

## âœ” Strict Grounding Prompt

The LLM is instructed to:

- Use only retrieved context
- Avoid hallucinations
- Respond with â€œI donâ€™t knowâ€ when information is missing

## ğŸ› ï¸ Tech Stack

- Python
- LangChain
- Ollama
- FAISS
- PyMuPDF
- Local LLMs (e.g., Llama3 / Mistral)

## âš™ï¸ Setup Instructions
## 1ï¸âƒ£ Install Ollama

Download from:
https://ollama.com

Verify:
ollama --version

## 2ï¸âƒ£ Pull a Local Model
ollama pull llama3

(You can replace with mistral or other supported models.)

## 3ï¸âƒ£ Create Virtual Environment
python -m venv venv

source venv/bin/activate     # Windows: venv\Scripts\activate

## 4ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

(Streamlit is included for future UI extension but not used in this version.)

## 5ï¸âƒ£ Configure PDF Path
place PDF file in data folder 

Update in main():
pdf_path = "data/filename.pdf"

## 6ï¸âƒ£ Run the Application
python app.py

Ask questions in the terminal.
Type exit to quit.

## âš ï¸ Limitations

- Performance depends on local hardware
- Local models may have weaker reasoning than cloud LLMs
- Context window limitations
- CLI-based interaction (no UI)

## ğŸš€ Future Enhancements

- Streamlit UI for interactive chat
- Support for multiple PDFs
- Persistent vector store
- Hybrid retrieval (BM25 + embeddings)
- Embedding caching
- Agentic RAG extensions

## ğŸ¯ Why No UI?

The UI layer is intentionally skipped to keep focus on:

- RAG architecture
- Retrieval quality
- Preprocessing impact
- Grounding strategies
UI can be added easily once the pipeline is solid.

## ğŸ¤ Closing Note

This project reflects hands-on learning and iteration, not a demo-first approach.
It is part of my broader effort to deeply understand how RAG systems behave in real conditions, especially when built locally.

Feedback and discussions are welcome.


